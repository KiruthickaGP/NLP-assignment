# -*- coding: utf-8 -*-
"""Kiruthicka_Gp_NLP_Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13cLGK46myYykmMRHMrZIysElTqGZ33U8
"""

! pip3 install transformers
! pip3 install datasets
! pip3 install accelerate
!pip install pyaudio
!pip install SpeechRecognition

from transformers import BertTokenizer, BertModel
import torch
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
from datasets import load_dataset
from google.colab import drive
import os
import warnings

drive.mount('/content/drive')

device = "cuda:0" if torch.cuda.is_available() else "cpu"
torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

model_id = "openai/whisper-large-v3"

model = AutoModelForSpeechSeq2Seq.from_pretrained(
    model_id, torch_dtype=torch_dtype, use_safetensors=True
)

model.to(device)

processor = AutoProcessor.from_pretrained(model_id)

pipe = pipeline(
    "automatic-speech-recognition",
    model=model,
    tokenizer=processor.tokenizer,
    feature_extractor=processor.feature_extractor,
    max_new_tokens=128,
    chunk_length_s=30,
    batch_size=16,
    return_timestamps=True,
    torch_dtype=torch_dtype,
    device=device,
)

from google.colab import drive
drive.mount('/content/drive')

import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="transformers.pipelines.base")

directory = '/content/drive/MyDrive/Audiofiles'


files = [os.path.join(directory, file) for file in os.listdir(directory) if os.path.isfile(os.path.join(directory, file))]


for file in files:
    result = pipe(file)
    print(result["text"])

import os

reference = '/content/drive/MyDrive/tran.txt'

with open(reference, 'r') as file:
    file_paths = file.readlines()

for file_path in file_paths:
    file_path = file_path.strip()
    if os.path.exists(file_path):
        with open(file_path, 'r') as file:
            text_content = file.read()
            print(text_content)
    else:
        print(file_path)

def calculate_wer(reference, hypothesis):
    ref_words = reference.split()
    hyp_words = hypothesis.split()

    substitutions = sum(1 for ref, hyp in zip(ref_words, hyp_words) if ref != hyp)
    deletions = len(ref_words) - len(hyp_words)
    insertions = len(hyp_words) - len(ref_words)

    total_words = len(ref_words)

    # Calculating the Word Error Rate (WER)
    wer = (substitutions + deletions + insertions) / total_words
    return wer

print(calculate_wer(reference, result["text"]))